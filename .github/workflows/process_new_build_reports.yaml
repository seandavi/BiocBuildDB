# Workflow derived from https://github.com/r-lib/actions/tree/v2/examples
# Need help debugging build failures? Start at https://github.com/r-lib/actions#where-to-find-help
on:
  schedule: 
    - cron: "30 23 * * *"
  # push:
  #   branches: [main, master]
  # pull_request:
  #   branches: [main, master]
  # release:
  #   types: [published]
  workflow_dispatch:

name: process_new_reports

jobs:
  process_new_reports:
    runs-on: ubuntu-latest
    # Only restrict concurrency for non-PR jobs
    concurrency:
      group: pkgdown-${{ github.event_name != 'pull_request' || github.run_id }}
    env:
      GITHUB_PAT: ${{ secrets.GITHUB_TOKEN }}
      ENV1: ${{ secrets.RCLONE_CONFIG }}
    permissions:
      contents: write
      # for aws s3 stuff
      id-token: write
    steps:
      - uses: actions/checkout@v4

      - name: Install rclone
        run: |
          curl https://rclone.org/install.sh | sudo bash        
      - name: rclone
        shell: bash
        env:
          RCLONE_CONFIG: "${{ secrets.RCLONE_CONFIG }}"
        run: |
          set -ex
          echo $RCLONE_CONFIG | base64 -d > rclone.conf
          rclone \
            --config rclone.conf \
            listremotes

      - name: Create report directory
        run: mkdir -p report_dir
        
      - name: rclone copy last_mod_date.csv
        run: 'rclone --config rclone.conf copy r2:bioconductor/checkResults/last_mod_date.csv report_dir/'

      - uses: r-lib/actions/setup-pandoc@v2

      - uses: r-lib/actions/setup-r@v2
        with:
          use-public-rspm: true

      - uses: r-lib/actions/setup-r-dependencies@v2
        with:
          extra-packages: any::pkgdown, any::devtools, local::.
          needs: website

      - name: "install package"
        run: devtools::install('.', dependencies = TRUE)
        shell: Rscript {0}

      - name: Process new build reports
        run: |
          library(BiocBuildDB)
          process_all_new_reports('report_dir/last_mod_date.csv', 'report_dir')
          list.files('report_dir')
        shell: Rscript {0}

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-region: us-east-1
          role-to-assume: arn:aws:iam::555219204010:role/bioc-s3writer-biocbuilddb
          role-session-name: GitHubActionsSession
          
      - name: Upload to S3
        run: aws s3 sync ./report_dir/ s3://bioc-builddb-mirror/buildResults/
        
      - name: Rclone copy results to cloud storage
        run: 'rclone --config rclone.conf copy report_dir/ r2:bioconductor/checkResults/'

      - name: Setup DuckDB
        uses: opt-nc/setup-duckdb-action@v1.2.0
        with:
          version: v1.4.3

      - name: parquet using duckdb
        run: |
          duckdb -c "INSTALL httpfs;
          LOAD httpfs;
          CREATE OR REPLACE SECRET s3_secret (
              TYPE S3,
              PROVIDER CREDENTIAL_CHAIN
          );
          COPY (SELECT * FROM read_csv_auto('s3://bioc-builddb-mirror/buildResults/*info.csv.gz')) TO 's3://bioc-builddb-mirror/parquet/info.parquet' (FORMAT PARQUET, compression zstd);
          COPY (SELECT * FROM read_csv_auto('s3://bioc-builddb-mirror/buildResults/*propagation_status.csv.gz')) TO 's3://bioc-builddb-mirror/parquet/propagation_status.parquet' (FORMAT PARQUET, compression zstd);
          COPY (SELECT * FROM read_csv_auto('s3://bioc-builddb-mirror/buildResults/*build_summary.csv.gz')) TO 's3://bioc-builddb-mirror/parquet/build_summary.parquet' (FORMAT PARQUET, compression zstd);"